{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a model for predicting the amount of population per area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we will upload the basic python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we will upload sklearn for statistical-learning for scientific data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.pipeline i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a class 'MyModel' which will hold all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel():    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing buildings with apartment two big or small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def area_vs_app_per_lev(self,data):\n",
    "        # creat ne column with the ratio of ORIG_AREA vs App_Per_Lvl\n",
    "        data['Area_per_App'] = data.ORIG_AREA/data.App_Per_Lvl\n",
    "        small_app = data.loc[data.Area_per_App < 40, :]\n",
    "    \n",
    "        big_app = data.loc[data.Area_per_App >= 200, :]\n",
    "        new_data = new_data = data.loc[(data.Area_per_App <= 200) & (data.Area_per_App > 40), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of the data using seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        sns.pairplot(data)\n",
    "        sns.pairplot(small_app)\n",
    "        sns.pairplot(big_app)\n",
    "        sns.pairplot(new_data)\n",
    "        small_app = small_app.drop('Area_per_App',axis=1)\n",
    "        big_app = big_app.drop('Area_per_App',axis=1)\n",
    "        new_data = new_data.drop('Area_per_App',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function returns all the small apartment(<40), the big apartments (>200) and the data between these values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        return small_app, big_app, new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat a function to predict the number of floors in each building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_num_floors(self,data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat new column of BLDG_Height vs Floors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        data['height_per_floors'] = data.BLDG_Height/data.Floors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate the data according to bad or good ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        bad_ratio_height_per_floors = data.loc[((data.height_per_floors < 3) | (data.height_per_floors > 5.5) | ((data.Floors != 1) & ((data.height_per_floors > 4.5) & (data.height_per_floors <= 5.5)))), :]\n",
    "        good_ratio_height_per_floors = data.loc[( ((data.height_per_floors >= 3) & (data.height_per_floors <= 4.5)) | (data.Floors == 1 & ((data.height_per_floors >= 4.5) & (data.height_per_floors <= 5.5)))), :]\n",
    "        no_floors = data.loc[np.isnan(data.Floors), :]\n",
    "        no_BLDG_Height = data.loc[np.isnan(data.BLDG_Height), :]\n",
    "        bad_ratio_height_per_floors_2 = pd.concat([bad_ratio_height_per_floors,no_floors],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of the data using seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        sns.pairplot(bad_ratio_height_per_floors)\n",
    "        sns.pairplot(good_ratio_height_per_floors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat a model to predict number of floors in a building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        x_train = good_ratio_height_per_floors['BLDG_Height']\n",
    "        y_train = good_ratio_height_per_floors['Floors']\n",
    "        x_test = bad_ratio_height_per_floors_2['BLDG_Height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        model = LinearRegression().fit(np.asarray(x_train).reshape(len(x_train), 1), y_train)\n",
    "        score_model_train = model.score(np.asarray(x).reshape(len(x), 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        predict_floor = model.predict(np.asarray(x_test).reshape(len(x_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round the numbers of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        round_predict_floor = np.rint(predict_floor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace the predictions with the old 'Floors' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        bad_ratio_height_per_floors_2 = bad_ratio_height_per_floors_2.drop(['Floors','height_per_floors'], axis=1)\n",
    "        bad_ratio_height_per_floors_2['Floors'] = round_predict_floor\n",
    "        good_ratio_height_per_floors = good_ratio_height_per_floors.drop(['height_per_floors'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the two data sets into one final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        final_data = pd.concat([bad_ratio_height_per_floors_2, good_ratio_height_per_floors], ignore_index=True)\n",
    "\n",
    "        return final_data,no_BLDG_Height\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function for removing of old buildings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def old_building(self,data):\n",
    "        # check for missing values (building without construction year)\n",
    "        missing_values=data.isnull().sum()\n",
    "        missing_year = data.loc[np.isnan(data.Construction_Year), :]\n",
    "\n",
    "        old_build = data.loc[data.Construction_Year < 1950, :]\n",
    "        old_build_2 = pd.concat([old_build,missing_year], ignore_index=True)\n",
    "        new_build = data.loc[data.Construction_Year >= 1950, :]\n",
    "\n",
    "        return old_build_2, new_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use skew metud to smoot values in the data with log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def skew_data(self,data):\n",
    "        # skew = data.skew()\n",
    "        new_data = data.copy()\n",
    "        # We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            for column in new_data.columns:\n",
    "                new_data[column] = np.log1p(np.asarray(new_data[column]))\n",
    "        else:\n",
    "            new_data = np.log1p(np.asarray(new_data))\n",
    "        return new_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat a function to get a list of models to evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_models(self):\n",
    "        models = dict()\n",
    "        models['lr'] = LinearRegression()\n",
    "        models['knn'] = KNeighborsRegressor()\n",
    "        models['rfr'] = RandomForestRegressor()\n",
    "        models['abr'] = AdaBoostRegressor()\n",
    "        models['gbr'] = GradientBoostingRegressor()\n",
    "        models['svm'] = make_pipeline(StandardScaler(), SVR())\n",
    "        models['stacking'] = obj1.get_stacking()\n",
    "        return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate a given model using cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate_model(self,model,x,y):\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate the prediction of given model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate_accuracy(self,y_true, y_pred):\n",
    "        l = len(y_pred)\n",
    "        match = 0\n",
    "        match_minus_1 = 0\n",
    "        match_plus_1 = 0\n",
    "        for i, j in zip(y_true,y_pred):\n",
    "            if i == j:\n",
    "                match += 1\n",
    "            elif i == j+1:\n",
    "                match_plus_1+=1\n",
    "            elif i == j-1:\n",
    "                match_minus_1+=1\n",
    "        return match/l, match_plus_1/l, match_minus_1/l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function to get a stacking ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_stacking(self):\n",
    "        # define the base models\n",
    "        level0 = list()\n",
    "        level0.append(('knn', KNeighborsRegressor()))\n",
    "        level0.append(('rfr', RandomForestRegressor()))\n",
    "        level0.append(('abr', AdaBoostRegressor()))\n",
    "        level0.append(('gbr', GradientBoostingRegressor()))\n",
    "        level0.append(('svm', make_pipeline(StandardScaler(), SVR())))\n",
    "        # define meta learner model\n",
    "        level1 = LinearRegression()\n",
    "        # define the stacking ensemble\n",
    "        model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function to avaluate best parameters for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def best_params(self, x, y, model, params):\n",
    "        gd_sr = GridSearchCV(estimator=model,param_grid=params,scoring='neg_mean_absolute_error',cv=5,n_jobs=-1)\n",
    "        gd_sr.fit(x,y)\n",
    "        return gd_sr.best_params_,gd_sr.best_estimator_, gd_sr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_path = r'\\BLDG_2020.xlsx'\n",
    "    all_data = pd.read_excel(file_path,sheet_name='Sheet1')\n",
    "    all_data_correct = pd.read_excel(file_path,sheet_name='Sheet2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of the data using seaborn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sns.pairplot(all_data_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the class MyModel where all functions is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    obj1=MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    small_ap,big_ap,all_data_for_model = obj1.area_vs_app_per_lev(all_data_correct)\n",
    "    all_data_for_model_2 = obj1.predict_num_floors(all_data_for_model)\n",
    "    old_buildings, all_data_for_model_3 = obj1.old_building(all_data_for_model_2)\n",
    "    remaining_data = pd.concat([small_ap, big_ap, old_buildings], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate values in Pandas dataframe column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    double_in_model = all_data_for_model_3['UNIQ_ID'].duplicated().any()\n",
    "    double_in_remainig = remaining_data['UNIQ_ID'].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the final data to reduce time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all_data_for_model_3.to_csv(path_or_buf=r'\\model_data.csv', index=False)\n",
    "    remaining_data.to_csv(path_or_buf=r'\\remaining_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload the data for the model after pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file_path_2 = r'\\model_data.csv'\n",
    "    all_data_for_model_4 = pd.read_csv(file_path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creat x and y for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all_data_x = pd.DataFrame(all_data_for_model_4, columns=['ORIG_AREA','BLDG_Height','Floors'])\n",
    "    all_data_y = all_data_for_model_4['App_Per_Lvl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_train, x_test, y_train, y_test = train_test_split(all_data_x, all_data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skew data using np.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_train_skew = obj1.skew_data(x_train)\n",
    "    x_test_skew = obj1.skew_data(x_test)\n",
    "    y_test_skew = obj1.skew_data(y_test)\n",
    "    y_train_skew = obj1.skew_data(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the models evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    models = obj1.get_models()\n",
    "    # evaluate the models and store results\n",
    "    results, names = list(), list()\n",
    "    for name, model in models.items():\n",
    "        scores = obj1.evaluate_model(model,x_train,y_train)\n",
    "        results.append(scores)\n",
    "        names.append(name)\n",
    "        print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "    # plot model performance for comparison\n",
    "    pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #  train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trained_meta_model = obj1.get_stacking()\n",
    "    trained_meta_model.fit(x_train_skew,y_train_skew)\n",
    "    predicted_y = np.rint(np.expm1(trained_meta_model.predict(x_test_skew)))\n",
    "    acc, acc_plus,acc_minus = obj1.evaluate_accuracy(np.asarray(y_test.tolist()),predicted_y)\n",
    "    evaluate_data = pd.DataFrame(x_test)\n",
    "    evaluate_data['true_y'] = y_test\n",
    "    evaluate_data['pre_y'] = predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    filename = r'\\finalized_model.sav'\n",
    "    joblib.dump(trained_meta_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat x and y for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    list_of_id = all_data_for_model_4['UNIQ_ID'].tolist()\n",
    "    all_data_2, no_BLDG_Height = obj1.predict_num_floors(all_data)\n",
    "    data_to_predict = all_data_2[~all_data_2.UNIQ_ID.isin(list_of_id)]\n",
    "    data_to_predict_x = pd.DataFrame(data_to_predict, columns=['ORIG_AREA','BLDG_Height','Floors'])\n",
    "\n",
    "    # skew data using np.log\n",
    "    data_to_predict_x_skew = obj1.skew_data(data_to_predict_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the final prediction of apartment for area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    final_prediction = np.expm1(loaded_model.predict(data_to_predict_x_skew))\n",
    "    final_round_prediction = np.rint(final_prediction)\n",
    "    data_to_predict = data_to_predict.drop(['App_Per_Lvl','AppCount'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add prediction to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_to_predict['App_Per_Lvl'] = final_prediction\n",
    "    data_to_predict['App_Per_Lvl_round'] = final_round_prediction\n",
    "\n",
    "    data_to_predict['AppCount'] = data_to_predict.Floors * data_to_predict.App_Per_Lvl\n",
    "    data_to_predict['AppCount_round'] = data_to_predict.Floors * data_to_predict.App_Per_Lvl_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all_data_3 = pd.concat([data_to_predict,all_data_for_model_4],ignore_index=True)\n",
    "    double_in_model = all_data_3['UNIQ_ID'].duplicated().any()\n",
    "    all_data_3.to_csv(path_or_buf=r'\\final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used GridSearchCV to better tuning the parameters for every model i used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # using Grid Search cv for better accuracy\n",
    "    knn_params = {'n_neighbors': [3, 4, 5, 6, 7, 8]}\n",
    "    gradient_boosting_params = {'max_depth':[3, 5 ,7],'min_samples_leaf': [1, 3, 5, 7],'learning_rate': [0.01, 0.1, 1, 10],'n_estimators': [100, 300, 500, 800]}\n",
    "    random_forest_param = {'n_estimators': [50, 100, 300, 500 ],'criterion': ['mse', 'mae'],'bootstrap': [True, False],'max_depth': [2, 4, 6, 8,10], 'min_samples_leaf': [1, 3, 5, 7]}\n",
    "    adaboost_param = {'learning_rate': [0.01, 0.1, 1, 10],'n_estimators': [50,100, 300, 500]}\n",
    "    svr_param = {'C': [1, 10, 100],'tol': [1e-3,1e-4, 1e-5],'epsilon': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "    knn_best_params,knn_best_estimator,knn_best_score = obj1.best_params(x_train,y_train,KNeighborsRegressor(),knn_params)\n",
    "    print(knn_best_params)\n",
    "#     best pamaters for KNeighborsRegressor is -{'n_neighbors': 8}\n",
    "\n",
    "    gradient_boosting_best_params,gradient_boosting_best_estimator,gradient_boosting_best_score = obj1.best_params(x_train, y_train, GradientBoostingRegressor(),gradient_boosting_params)\n",
    "    print(gradient_boosting_best_params)\n",
    "#     best pamaters 2 for GradientBoostingRegressor is\n",
    "\n",
    "    random_forest_best_params,random_forest_best_estimator,random_forest_best_score = obj1.best_params(x_train, y_train, RandomForestRegressor(), random_forest_param)\n",
    "    print(random_forest_best_params)\n",
    "#     best pamaters  for RandomForestRegressor is\n",
    "\n",
    "    adaboost_best_params,adaboost_best_estimator,adaboost_best_score = obj1.best_params(x_train, y_train, AdaBoostRegressor(), adaboost_param)\n",
    "    print(adaboost_best_params)\n",
    "#     best pamaters  for AdaBoostRegressor is {'learning_rate': 0.01, 'n_estimators': 100}\n",
    "\n",
    "    svr_best_params,svr_best_estimator,svr_best_score = obj1.best_params(x_train, y_train, make_pipeline(StandardScaler(), SVR()), svr_param)\n",
    "    print(svr_best_params)\n",
    "#     best pamaters  for SVR is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
